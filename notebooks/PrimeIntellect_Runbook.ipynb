{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introspect: Prime Intellect GPU Runbook\n",
        "\n",
        "This notebook orchestrates end-to-end runs of the Introspect toolkit on a GPU server.\n",
        "It installs dependencies (editable), validates the GPU/dtype, builds concept vectors,\n",
        "then executes Tasks A–D and optional sweeps/plots.\n",
        "\n",
        "Notes:\n",
        "- Defaults select `bfloat16` on GPUs that support it; otherwise `float16`.\n",
        "- No 4-bit quantization is used. Device placement relies on `device_map=\"auto\"`.\n",
        "- Adjust model IDs and counts to match your available VRAM.\n",
        "- For private models, set the `HUGGING_FACE_HUB_TOKEN` env var before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "# Environment setup\n",
        "import os, sys, json, subprocess, shlex, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path.cwd()  # run notebook from repo root\n",
        "print('Repo root:', REPO_ROOT)\n",
        "print('Python:', sys.version)\n",
        "\n",
        "# Optional: point HF cache to a fast disk\n",
        "os.environ.setdefault('HF_HOME', str(REPO_ROOT / '.hf-cache'))\n",
        "os.environ.setdefault('TRANSFORMERS_CACHE', str(REPO_ROOT / '.hf-cache' / 'hub'))\n",
        "print('HF_HOME=', os.environ['HF_HOME'])\n",
        "\n",
        "# Ensure local package is on path for module execution\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "def run(cmd: list[str] | str, env=None):\n",
        "    if isinstance(cmd, str):\n",
        "        print('$', cmd)\n",
        "        result = subprocess.run(cmd, shell=True, env=env, check=True)\n",
        "    else:\n",
        "        print('$', ' '.join(shlex.quote(c) for c in cmd))\n",
        "        result = subprocess.run(cmd, env=env, check=True)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "pip"
        ]
      },
      "outputs": [],
      "source": [
        "# Install in editable mode with optional Hydra support\n",
        "# Skip if already installed in your environment.\n",
        "try:\n",
        "    import introspect  # noqa: F401\n",
        "    print('introspect already importable; skipping install.')\n",
        "except Exception:\n",
        "    run([sys.executable, '-m', 'pip', 'install', '-e', '.[hydra]'])\n",
        "\n",
        "# Print key dependency versions\n",
        "import importlib\n",
        "for pkg in ['torch', 'transformers', 'accelerate']:\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        print(f'{pkg}=={getattr(mod, "__version__", "?")}')\n",
        "    except Exception as e:\n",
        "        print(f'Warning: {pkg} not importable: {e}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU / dtype check\n",
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('Device count:', torch.cuda.device_count())\n",
        "    print('Current device:', torch.cuda.current_device())\n",
        "    print('Device name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "BF16_OK = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "DTYPE = 'bf16' if BF16_OK else ('fp16' if torch.cuda.is_available() else 'fp32')\n",
        "print('Selected dtype:', DTYPE)\n",
        "\n",
        "# Optional: verify nvidia-smi output\n",
        "try:\n",
        "    run('nvidia-smi')\n",
        "except Exception as e:\n",
        "    print('nvidia-smi not available or failed:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Runtime parameters\n",
        "Adjust these to match your server capacity and desired coverage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "params"
        ]
      },
      "outputs": [],
      "source": [
        "# Models present in introspect/registry/models.yaml\n",
        "MODEL_ID = 'meta-llama/Llama-3-8b'             # change as needed\n",
        "LAYERS_VECTOR = [8, 16, 24, 32]                # for vector building\n",
        "LAYERS_TASK_A = [8, 16, 24, 32]                # Task A layers\n",
        "ALPHAS_TASK_A = [1, 2, 4, 8, 16]               # Task A scales\n",
        "TASK_A_N_CONCEPTS = 10                         # reduce for quick pass\n",
        "\n",
        "LAYERS_TASK_B = [16, 24]                       # Task B layers\n",
        "TASK_B_ALPHA = 4.0\n",
        "TASK_B_TRIALS = 10\n",
        "\n",
        "LAYERS_TASK_C = [12, 24]                       # Task C layers\n",
        "TASK_C_ALPHA = 6.0\n",
        "TASK_C_TRIALS = 10\n",
        "\n",
        "LAYERS_TASK_D = [0, 8, 16, 24, 32]             # Task D layers\n",
        "TASK_D_CONCEPTS = 10\n",
        "\n",
        "WORDS_FILE = 'introspect/concepts/words.yaml'\n",
        "CACHE_DIR = 'introspect/results/vectors'\n",
        "RESULTS_DIR = 'introspect/results'\n",
        "SEED = 42\n",
        "DEVICE_MAP = 'auto'                             # or 'cpu'\n",
        "DTYPE_OVERRIDE = DTYPE                          # bf16/fp16/fp32\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Hugging Face login\n",
        "If your model access requires authentication, expose `HUGGING_FACE_HUB_TOKEN` in env.\n",
        "Skip if models are public or credentials are already configured.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: set token from a secret or input (do not hardcode in notebooks).\n",
        "# import getpass\n",
        "# os.environ['HUGGING_FACE_HUB_TOKEN'] = getpass.getpass('HF token: ')\n",
        "print('HF token present:', bool(os.environ.get('HUGGING_FACE_HUB_TOKEN')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Build concept vectors\n",
        "Caches vectors to `introspect/results/vectors/<model>/layer####_<word>.npy` with metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "vectors"
        ]
      },
      "outputs": [],
      "source": [
        "args = [\n",
        "    sys.executable, '-m', 'introspect.src.vectors',\n",
        "    '--model', MODEL_ID,\n",
        "    '--layers', *list(map(str, LAYERS_VECTOR)),\n",
        "    '--words-file', WORDS_FILE,\n",
        "    '--limit-targets', '10',\n",
        "    '--cache-dir', CACHE_DIR,\n",
        "    '--device-map', DEVICE_MAP,\n",
        "]\n",
        "# Only pass dtype override when not relying on registry\n",
        "if DTYPE_OVERRIDE:\n",
        "    args += ['--dtype', DTYPE_OVERRIDE]\n",
        "run(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Task A — Injected report detection\n",
        "Runs control, negative, random, and target variants across layers×alpha for N concepts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "task-a"
        ]
      },
      "outputs": [],
      "source": [
        "task_a_out = str(Path(RESULTS_DIR) / 'task_A.jsonl')\n",
        "args = [\n",
        "    sys.executable, '-m', 'introspect.src.eval_A_injected_report',\n",
        "    '--model', MODEL_ID,\n",
        "    '--layers', *list(map(str, LAYERS_TASK_A)),\n",
        "    '--alphas', *list(map(str, ALPHAS_TASK_A)),\n",
        "    '--n-concepts', str(TASK_A_N_CONCEPTS),\n",
        "    '--results-path', task_a_out,\n",
        "    '--cache-dir', CACHE_DIR,\n",
        "    '--words-file', WORDS_FILE,\n",
        "    '--seed', str(SEED),\n",
        "]\n",
        "if DTYPE_OVERRIDE: args += ['--dtype', DTYPE_OVERRIDE]\n",
        "if DEVICE_MAP: args += ['--device-map', DEVICE_MAP]\n",
        "run(args)\n",
        "print('Task A results →', task_a_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Task B — Thought vs. Text\n",
        "Evaluates thought word, exact repetition, and multiple-choice alignment with and without injection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "task-b"
        ]
      },
      "outputs": [],
      "source": [
        "task_b_out = str(Path(RESULTS_DIR) / 'task_B.jsonl')\n",
        "args = [\n",
        "    sys.executable, '-m', 'introspect.src.eval_B_thoughts_vs_text',\n",
        "    '--model', MODEL_ID,\n",
        "    '--layers', *list(map(str, LAYERS_TASK_B)),\n",
        "    '--alpha', str(TASK_B_ALPHA),\n",
        "    '--n-trials', str(TASK_B_TRIALS),\n",
        "    '--results-path', task_b_out,\n",
        "    '--cache-dir', CACHE_DIR,\n",
        "    '--words-file', WORDS_FILE,\n",
        "    '--seed', str(SEED),\n",
        "]\n",
        "if DTYPE_OVERRIDE: args += ['--dtype', DTYPE_OVERRIDE]\n",
        "if DEVICE_MAP: args += ['--device-map', DEVICE_MAP]\n",
        "run(args)\n",
        "print('Task B results →', task_b_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Task C — Prefill Intent\n",
        "Measures intent labelling with and without retro-injection near a prefilled word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "task-c"
        ]
      },
      "outputs": [],
      "source": [
        "task_c_out = str(Path(RESULTS_DIR) / 'task_C.jsonl')\n",
        "args = [\n",
        "    sys.executable, '-m', 'introspect.src.eval_C_prefill_intent',\n",
        "    '--model', MODEL_ID,\n",
        "    '--layers', *list(map(str, LAYERS_TASK_C)),\n",
        "    '--alpha', str(TASK_C_ALPHA),\n",
        "    '--n-trials', str(TASK_C_TRIALS),\n",
        "    '--results-path', task_c_out,\n",
        "    '--cache-dir', CACHE_DIR,\n",
        "    '--words-file', WORDS_FILE,\n",
        "    '--seed', str(SEED),\n",
        "]\n",
        "if DTYPE_OVERRIDE: args += ['--dtype', DTYPE_OVERRIDE]\n",
        "if DEVICE_MAP: args += ['--device-map', DEVICE_MAP]\n",
        "run(args)\n",
        "print('Task C results →', task_c_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Task D — Intentional Control Curves\n",
        "Captures per-layer cosine activation deltas between think vs. don't-think prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "task-d"
        ]
      },
      "outputs": [],
      "source": [
        "task_d_out = str(Path(RESULTS_DIR) / 'task_D.jsonl')\n",
        "args = [\n",
        "    sys.executable, '-m', 'introspect.src.eval_D_intentional_control',\n",
        "    '--model', MODEL_ID,\n",
        "    '--layers', *list(map(str, LAYERS_TASK_D)),\n",
        "    '--n-concepts', str(TASK_D_CONCEPTS),\n",
        "    '--results-path', task_d_out,\n",
        "    '--cache-dir', CACHE_DIR,\n",
        "    '--words-file', WORDS_FILE,\n",
        "    '--seed', str(SEED),\n",
        "]\n",
        "if DTYPE_OVERRIDE: args += ['--dtype', DTYPE_OVERRIDE]\n",
        "if DEVICE_MAP: args += ['--device-map', DEVICE_MAP]\n",
        "run(args)\n",
        "print('Task D results →', task_d_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Sweep and plots (optional)\n",
        "Generates per-model task outputs and plots under `introspect/results/<model_slug>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "sweep"
        ]
      },
      "outputs": [],
      "source": [
        "RUN_SWEEP = False  # set True to enable\n",
        "if RUN_SWEEP:\n",
        "    args = [\n",
        "        sys.executable, '-m', 'introspect.src.sweep',\n",
        "        '--models', MODEL_ID,\n",
        "        '--tasks', 'A', 'B', 'C', 'D',\n",
        "        '--layer-grid', '10',\n+        "        '--task-a-concepts', '10',\n+        "        '--task-b-trials', '10',\n+        "        '--task-c-trials', '10',\n+        "        '--task-d-concepts', '10',\n+        "        '--run-plots',\n+        "    ]\n",
        "    run(args)\n",
        "else:\n",
        "    print('Sweep disabled; set RUN_SWEEP=True to enable.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Plot-only refresh (from existing results)\n",
        "Rebuilds plots without rerunning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "plots"
        ]
      },
      "outputs": [],
      "source": [
        "PLOT_ONLY = False  # set True to refresh plots\n",
        "if PLOT_ONLY:\n",
        "    args = [\n",
        "        sys.executable, '-m', 'introspect.src.sweep',\n",
        "        '--models', MODEL_ID,\n",
        "        '--plot-only',\n",
        "        '--results-root', RESULTS_DIR,\n",
        "    ]\n",
        "    run(args)\n",
        "else:\n",
        "    print('Plot-only disabled; set PLOT_ONLY=True to enable.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

